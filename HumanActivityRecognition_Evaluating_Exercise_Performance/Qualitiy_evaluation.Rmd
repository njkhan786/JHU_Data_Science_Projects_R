---
title: "Predicting exercise quality with random forest"
output: html_document
---

This is a course project for the practical machine learning class from JHU on Coursera. The data for this project is published in a conference proceeding: Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. The data is kindly made available by the authors at <http://groupware.les.inf.puc-rio.br/har>.

Emma Yu

Jan 2015


#### Introduction

Many effort has been put into recognizing the type of exciecising, but people don't usually evaluate the correctness and effectiveness of the exercise. The goal of this project is to use controled data to train the algorithm to learn to evaluate exercise quality. Data is gathered from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 


#### Data Processing

First, let's download the training and testing data set from: 
```{r, eval=FALSE}
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', 
              "pml-training.csv", method = "curl")
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', 
              "pml-testing.csv", method = "curl")

```


In my experiments, I found using 10% of the training dataset provides 88% prediction accuracy.  Since increasing the amount of training data will increase the running time dramatically, and the 88% accuracy is already acceptable. I decided to use only 10 percent of the training data set to build our model for prediction.

```{r, eval=TRUE}
# Subset 10% of the data
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

library(caret)
insmall <- createDataPartition(y = training$classe,
                               p = 0.90, list = FALSE)
smdata <- training[-insmall,]

# First only extract accelerator data
names <- names(smdata)
acc <- grep('acc', names)
acc <- c(acc, grep('classe', names))
smacc <- smdata[acc]
#head(smacc)

namesacc <- names(smacc)
varacc <- grep('var', namesacc)
smacc <- smacc[-varacc]
#head(smacc)
```

#### Building the model

After several experiment, I found random forest can achieve decent performace with a relative small amount of data. The algorithm takes 367.261s in total to process.

```{r, cache=TRUE}
system.time(modFit <- train(classe ~., data = smacc, method ='rf', prox = TRUE))
modFit$finalModel
```

#### Model testing

We testing the algorithm with test data ramdomply selected from the original dataset, and clean the data as the training set.

```{r, cache=TRUE}
test <- createDataPartition(y = training$classe,
                            p = 0.9, list = FALSE)
testdata <- training[-test,]

# clean just as the training dataset
names <- names(testdata)
acc <- grep('acc', names)
acc <- c(acc, grep('classe', names))
smtest <- testdata[,acc]

namesacc <- names(smtest)
vartest <- grep('var', namesacc)
smtest <- smtest[-vartest]
#head(smtest)

pred <- predict(modFit, smtest)
smtest$predRight <- pred == smtest$classe
table(pred, smtest$classe)
```

The estimation of the out-of-sample accurracy is:
```{r}
sum(smtest$predRight)/1960.
```


#### Model prediction

```{r}
# clean just as the training dataset
names <- names(testing)
acc <- grep('acc', names)
acc <- c(acc, grep('classe', names))
smtesting <- testing[,acc]

namesacc <- names(smtesting)
vartest <- grep('var', namesacc)
smtestting <- smtesting[-vartest]
#head(smtest)

answers <- predict(modFit, smtesting)
```

```{r}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```
